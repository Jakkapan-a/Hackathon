# ğŸš€ Workflow à¸à¸²à¸£à¸—à¸³ Hackathon: Digitize NACC Asset Declaration

## ğŸ“‹ à¸ à¸²à¸à¸£à¸§à¸¡ Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Test_doc_info    â”‚ â† à¸ˆà¸¸à¸”à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™
â”‚  (à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­ PDF)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œ PDF     â”‚
â”‚  à¸ˆà¸²à¸ path/URL        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. PDF â†’ Image      â”‚
â”‚  à¹à¸›à¸¥à¸‡à¸—à¸¸à¸à¸«à¸™à¹‰à¸²à¹€à¸›à¹‡à¸™à¸£à¸¹à¸›  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. OCR              â”‚ â† Tesseract/PyTesseract
â”‚  Image â†’ Text        â”‚   à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸ à¸²à¸©à¸²à¹„à¸—à¸¢
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. LLM Parsing      â”‚ â† Ollama
â”‚  Text â†’ Structured   â”‚   à¹à¸¢à¸à¸«à¸¡à¸§à¸”à¸«à¸¡à¸¹à¹ˆ + Map Enum
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. Generate CSV     â”‚ â† à¸ªà¸£à¹‰à¸²à¸‡ 13 à¹„à¸Ÿà¸¥à¹Œ
â”‚  13 Output Files     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. Validation       â”‚ â† à¸£à¸±à¸™ validation_query.sql
â”‚  Run SQL â†’ Summary   â”‚   à¸ªà¸£à¹‰à¸²à¸‡ summary.csv
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  8. Submit           â”‚ â† à¸ªà¹ˆà¸‡ Kaggle
â”‚  Upload to Kaggle    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¹à¸•à¹ˆà¸¥à¸°à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™

### ğŸ”µ Step 1: à¹€à¸£à¸´à¹ˆà¸¡à¸ˆà¸²à¸ Test_doc_info.csv

```python
import pandas as pd

# à¸­à¹ˆà¸²à¸™à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­ PDF à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
doc_info = pd.read_csv("Test_doc_info.csv", encoding="utf-8-sig")

# Structure:
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ doc_id â”‚ doc_location_url                 â”‚ type_id â”‚ nacc_id â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ 2098   â”‚ à¸ˆà¸¸à¸•à¸´_à¹„à¸à¸£à¸¤à¸à¸©à¹Œ_à¸ª.à¸ª._à¸à¸£à¸“à¸µ....pdf   â”‚    1    â”‚  1970   â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# à¹„à¸”à¹‰ 9 à¹„à¸Ÿà¸¥à¹Œ PDF à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥
```

**Output**: à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­ PDF à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” à¸à¸£à¹‰à¸­à¸¡ nacc_id à¸ªà¸³à¸«à¸£à¸±à¸š mapping

---

### ğŸ”µ Step 2: à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œ PDF

```python
from pathlib import Path

for idx, row in doc_info.iterrows():
    pdf_filename = row['doc_location_url']
    nacc_id = row['nacc_id']
    
    # Path à¸‚à¸­à¸‡ PDF (à¸›à¸£à¸±à¸šà¸•à¸²à¸¡à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¸ˆà¸£à¸´à¸‡)
    pdf_path = Path(f"./Test_pdf/{pdf_filename}")
    
    print(f"Processing: {pdf_filename}")
    print(f"  nacc_id: {nacc_id}")
```

**à¸—à¸µà¹ˆà¸•à¸±à¹‰à¸‡à¹„à¸Ÿà¸¥à¹Œ**: à¸•à¸²à¸¡à¸—à¸µà¹ˆ Kaggle à¸ˆà¸±à¸”à¹ƒà¸«à¹‰ (à¸­à¸²à¸ˆà¹€à¸›à¹‡à¸™ ZIP à¸•à¹‰à¸­à¸‡ extract à¸à¹ˆà¸­à¸™)

---

### ğŸ”µ Step 3: PDF â†’ Image

```python
import fitz  # PyMuPDF
from PIL import Image

def pdf_to_images(pdf_path):
    """à¹à¸›à¸¥à¸‡ PDF à¸—à¸¸à¸à¸«à¸™à¹‰à¸²à¹€à¸›à¹‡à¸™ images"""
    
    pdf_document = fitz.open(pdf_path)
    images = []
    
    for page_num in range(pdf_document.page_count):
        page = pdf_document[page_num]
        
        # à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ image (300 DPI à¸ªà¸³à¸«à¸£à¸±à¸š OCR à¸—à¸µà¹ˆà¸”à¸µ)
        pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72))
        
        # à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ PIL Image
        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        images.append(img)
    
    pdf_document.close()
    return images

# à¹ƒà¸Šà¹‰à¸‡à¸²à¸™
images = pdf_to_images(pdf_path)
print(f"  à¸ˆà¸³à¸™à¸§à¸™à¸«à¸™à¹‰à¸²: {len(images)} à¸«à¸™à¹‰à¸²")
```

**Output**: List à¸‚à¸­à¸‡ images à¹à¸•à¹ˆà¸¥à¸°à¸«à¸™à¹‰à¸²

---

### ğŸ”µ Step 4: OCR (Image â†’ Text)

```python
import pytesseract
from PIL import Image

def ocr_image(image, lang='tha+eng'):
    """
    à¸—à¸³ OCR à¸šà¸™ image
    lang='tha+eng' = à¸£à¸­à¸‡à¸£à¸±à¸šà¸—à¸±à¹‰à¸‡à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹à¸¥à¸°à¸­à¸±à¸‡à¸à¸¤à¸©
    """
    
    # Config à¸ªà¸³à¸«à¸£à¸±à¸š OCR
    custom_config = r'--oem 3 --psm 6'
    
    # à¸”à¸¶à¸‡ text à¸­à¸­à¸à¸¡à¸²
    text = pytesseract.image_to_string(
        image, 
        lang=lang, 
        config=custom_config
    )
    
    return text.strip()

# à¹ƒà¸Šà¹‰à¸‡à¸²à¸™
all_text = []
for page_num, img in enumerate(images, 1):
    text = ocr_image(img)
    all_text.append({
        'page': page_num,
        'text': text
    })
    print(f"  Page {page_num}: {len(text)} characters")

# à¸£à¸§à¸¡ text à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
full_text = "\n\n".join([page['text'] for page in all_text])
```

**Output**: Text à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸ˆà¸²à¸à¸—à¸¸à¸à¸«à¸™à¹‰à¸² PDF

---

### ğŸ”µ Step 5: LLM Parsing (Text â†’ Structured Data)

```python
import requests
import json

def parse_with_llm(ocr_text, nacc_id, section="personal_info"):
    """
    à¸ªà¹ˆà¸‡ OCR text à¹„à¸›à¹ƒà¸«à¹‰ LLM parse
    """
    
    # à¸­à¹ˆà¸²à¸™ enum types à¸ªà¸³à¸«à¸£à¸±à¸š context
    enums = load_enum_context()  # à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¹‚à¸«à¸¥à¸” enum à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
    
    # à¸ªà¸£à¹‰à¸²à¸‡ prompt
    prompt = f"""
à¸„à¸¸à¸“à¹€à¸›à¹‡à¸™ AI à¸—à¸µà¹ˆà¹€à¸Šà¸µà¹ˆà¸¢à¸§à¸Šà¸²à¸à¹ƒà¸™à¸à¸²à¸£à¹à¸›à¸¥à¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¸šà¸±à¸à¸Šà¸µà¸—à¸£à¸±à¸à¸¢à¹Œà¸ªà¸´à¸™à¸‚à¸­à¸‡ à¸›.à¸›.à¸Š. à¹€à¸›à¹‡à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸¡à¸µà¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡

NACC_ID: {nacc_id}

à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸ˆà¸²à¸ OCR:
{ocr_text}

à¸à¸£à¸¸à¸“à¸²à¹à¸¢à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¹ˆà¸§à¸™ "{section}" à¸­à¸­à¸à¸¡à¸²à¹€à¸›à¹‡à¸™ JSON à¹‚à¸”à¸¢:
1. à¹ƒà¸Šà¹‰ enum ID à¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡à¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸™à¸µà¹‰:
{json.dumps(enums[section], ensure_ascii=False, indent=2)}

2. à¸«à¹‰à¸²à¸¡à¹€à¸”à¸² enum ID - à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¹à¸™à¹ˆà¹ƒà¸ˆà¹ƒà¸«à¹‰à¹ƒà¸ªà¹ˆ null
3. à¸ˆà¸±à¸”à¸£à¸¹à¸›à¹à¸šà¸šà¸§à¸±à¸™à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™ YYYY-MM-DD

à¸•à¸­à¸šà¸à¸¥à¸±à¸šà¹€à¸›à¹‡à¸™ JSON à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸¡à¸µà¸„à¸³à¸­à¸˜à¸´à¸šà¸²à¸¢
"""
    
    # à¹€à¸£à¸µà¸¢à¸ Ollama API
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": "typhoon",
            "prompt": prompt,
            "temperature": 0.1,
            "max_tokens": 4000,
            "stream": False
        }
    )
    
    result = response.json()
    
    # Clean up à¹à¸¥à¸° parse JSON
    json_text = result['response'].strip()
    json_text = json_text.replace("```json", "").replace("```", "").strip()
    
    parsed_data = json.loads(json_text)
    
    return parsed_data

# à¹ƒà¸Šà¹‰à¸‡à¸²à¸™ - parse à¹à¸•à¹ˆà¸¥à¸° section
sections = [
    "personal_info",      # à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¹ˆà¸§à¸™à¸•à¸±à¸§
    "spouse_info",        # à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸„à¸¹à¹ˆà¸ªà¸¡à¸£à¸ª
    "relatives",          # à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸à¸²à¸•à¸´
    "positions",          # à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡
    "income_statement",   # à¸£à¸²à¸¢à¹„à¸”à¹‰
    "expense_statement",  # à¸£à¸²à¸¢à¸ˆà¹ˆà¸²à¸¢
    "tax_statement",      # à¸ à¸²à¸©à¸µ
    "assets_land",        # à¸—à¸µà¹ˆà¸”à¸´à¸™
    "assets_building",    # à¸ªà¸´à¹ˆà¸‡à¸›à¸¥à¸¹à¸à¸ªà¸£à¹‰à¸²à¸‡
    "assets_vehicle",     # à¸¢à¸²à¸™à¸à¸²à¸«à¸™à¸°
    "assets_other"        # à¸—à¸£à¸±à¸à¸¢à¹Œà¸ªà¸´à¸™à¸­à¸·à¹ˆà¸™
]

parsed_results = {}
for section in sections:
    parsed_results[section] = parse_with_llm(
        ocr_text=full_text,
        nacc_id=nacc_id,
        section=section
    )
```

**Output**: à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸¡à¸µà¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹à¸¥à¹‰à¸§ à¸à¸£à¹‰à¸­à¸¡ enum ID à¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡

---

### ğŸ”µ Step 6: Generate CSV (13 à¹„à¸Ÿà¸¥à¹Œ)

```python
def generate_csv_files(parsed_results, nacc_id):
    """
    à¸ªà¸£à¹‰à¸²à¸‡ CSV 13 à¹„à¸Ÿà¸¥à¹Œà¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆ parse à¹à¸¥à¹‰à¸§
    """
    
    output_dir = Path(f"output_{nacc_id}")
    output_dir.mkdir(exist_ok=True)
    
    # 1. submitter_old_name.csv
    if parsed_results['personal_info'].get('old_names'):
        df_old_name = pd.DataFrame(parsed_results['personal_info']['old_names'])
        df_old_name.to_csv(
            output_dir / "submitter_old_name.csv",
            index=False,
            encoding="utf-8-sig"
        )
    
    # 2. submitter_position.csv
    df_position = pd.DataFrame(parsed_results['positions'])
    df_position.to_csv(
        output_dir / "submitter_position.csv",
        index=False,
        encoding="utf-8-sig"
    )
    
    # 3. spouse_info.csv
    if parsed_results['spouse_info']:
        df_spouse = pd.DataFrame([parsed_results['spouse_info']])
        df_spouse.to_csv(
            output_dir / "spouse_info.csv",
            index=False,
            encoding="utf-8-sig"
        )
    
    # ... à¸ªà¸£à¹‰à¸²à¸‡à¸•à¹ˆà¸­à¹„à¸›à¸ˆà¸™à¸„à¸£à¸š 13 à¹„à¸Ÿà¸¥à¹Œ
    
    # 9. asset.csv (à¸£à¸§à¸¡à¸—à¸£à¸±à¸à¸¢à¹Œà¸ªà¸´à¸™à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”)
    all_assets = []
    all_assets.extend(parsed_results['assets_land'])
    all_assets.extend(parsed_results['assets_building'])
    all_assets.extend(parsed_results['assets_vehicle'])
    all_assets.extend(parsed_results['assets_other'])
    
    df_assets = pd.DataFrame(all_assets)
    df_assets.to_csv(
        output_dir / "asset.csv",
        index=False,
        encoding="utf-8-sig"
    )
    
    print(f"âœ… à¸ªà¸£à¹‰à¸²à¸‡ CSV 13 à¹„à¸Ÿà¸¥à¹Œà¹€à¸ªà¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§ à¹ƒà¸™ {output_dir}/")

# à¹ƒà¸Šà¹‰à¸‡à¸²à¸™
generate_csv_files(parsed_results, nacc_id)
```

**Output**: 13 à¹„à¸Ÿà¸¥à¹Œ CSV à¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ `output_{nacc_id}/`

### à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­ 13 à¹„à¸Ÿà¸¥à¹Œ:
1. âœ… submitter_old_name.csv
2. âœ… submitter_position.csv
3. âœ… spouse_info.csv
4. âœ… spouse_old_name.csv
5. âœ… spouse_position.csv
6. âœ… relative_info.csv
7. âœ… statement.csv
8. âœ… statement_detail.csv
9. âœ… asset.csv
10. âœ… asset_building_info.csv
11. âœ… asset_land_info.csv
12. âœ… asset_vehicle_info.csv
13. âœ… asset_other_asset_info.csv

---

### ğŸ”µ Step 7: Validation (Run SQL)

```python
import sqlite3

def validate_and_create_summary(output_dir):
    """
    à¸£à¸±à¸™ validation_query.sql à¹€à¸à¸·à¹ˆà¸­à¸ªà¸£à¹‰à¸²à¸‡ summary.csv
    """
    
    # à¸ªà¸£à¹‰à¸²à¸‡ in-memory SQLite database
    conn = sqlite3.connect(':memory:')
    
    # à¹‚à¸«à¸¥à¸” CSV à¸—à¸±à¹‰à¸‡ 13 à¹„à¸Ÿà¸¥à¹Œà¹€à¸‚à¹‰à¸² SQLite
    csv_files = {
        'submitter_old_name': 'submitter_old_name.csv',
        'submitter_position': 'submitter_position.csv',
        'spouse_info': 'spouse_info.csv',
        'spouse_old_name': 'spouse_old_name.csv',
        'spouse_position': 'spouse_position.csv',
        'relative_info': 'relative_info.csv',
        'statement': 'statement.csv',
        'statement_detail': 'statement_detail.csv',
        'asset': 'asset.csv',
        'asset_building_info': 'asset_building_info.csv',
        'asset_land_info': 'asset_land_info.csv',
        'asset_vehicle_info': 'asset_vehicle_info.csv',
        'asset_other_asset_info': 'asset_other_asset_info.csv'
    }
    
    for table_name, csv_file in csv_files.items():
        csv_path = output_dir / csv_file
        if csv_path.exists():
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
            df.to_sql(table_name, conn, if_exists='replace', index=False)
    
    # à¸­à¹ˆà¸²à¸™ validation_query.sql
    with open('validation_query.sql', 'r', encoding='utf-8') as f:
        sql_query = f.read()
    
    # à¸£à¸±à¸™ query
    df_summary = pd.read_sql_query(sql_query, conn)
    
    # à¸šà¸±à¸™à¸—à¸¶à¸ summary.csv
    df_summary.to_csv(
        output_dir / "summary.csv",
        index=False,
        encoding="utf-8-sig"
    )
    
    conn.close()
    
    print(f"âœ… à¸ªà¸£à¹‰à¸²à¸‡ summary.csv à¹€à¸ªà¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§")
    return df_summary

# à¹ƒà¸Šà¹‰à¸‡à¸²à¸™
summary = validate_and_create_summary(output_dir)
```

**Output**: summary.csv à¸ªà¸³à¸«à¸£à¸±à¸š submit

---

### ğŸ”µ Step 8: Submit to Kaggle

```python
def prepare_submission():
    """
    à¸£à¸§à¸¡ summary à¸ˆà¸²à¸à¸—à¸¸à¸ nacc_id à¹€à¸›à¹‡à¸™à¹„à¸Ÿà¸¥à¹Œà¹€à¸”à¸µà¸¢à¸§
    """
    
    all_summaries = []
    
    # à¸­à¹ˆà¸²à¸™ summary à¸ˆà¸²à¸à¸—à¸¸à¸ output directory
    for output_dir in Path('.').glob('output_*'):
        summary_file = output_dir / 'summary.csv'
        if summary_file.exists():
            df = pd.read_csv(summary_file, encoding='utf-8-sig')
            all_summaries.append(df)
    
    # à¸£à¸§à¸¡à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
    final_summary = pd.concat(all_summaries, ignore_index=True)
    
    # à¸šà¸±à¸™à¸—à¸¶à¸
    final_summary.to_csv(
        'submission_summary.csv',
        index=False,
        encoding='utf-8-sig'
    )
    
    print(f"âœ… à¸ªà¸£à¹‰à¸²à¸‡ submission_summary.csv à¹€à¸ªà¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§ ({len(final_summary)} rows)")

# à¹ƒà¸Šà¹‰à¸‡à¸²à¸™
prepare_submission()
```

**Output**: `submission_summary.csv` à¸à¸£à¹‰à¸­à¸¡ submit à¹ƒà¸™ Kaggle

---

## ğŸ¯ à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸„à¹‰à¸”à¹à¸™à¸°à¸™à¸³

```
project/
â”œâ”€â”€ main.py                      # Script à¸«à¸¥à¸±à¸
â”œâ”€â”€ config.py                    # Configuration
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ pdf_processor.py         # PDF â†’ Image
â”‚   â”œâ”€â”€ ocr_engine.py            # OCR â†’ Text
â”‚   â”œâ”€â”€ llm_parser.py            # LLM Parsing
â”‚   â”œâ”€â”€ csv_generator.py         # Generate CSV
â”‚   â””â”€â”€ validator.py             # Validation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Test_doc_info.csv
â”‚   â”œâ”€â”€ Test_nacc_detail.csv
â”‚   â”œâ”€â”€ Test_submitter_info.csv
â”‚   â”œâ”€â”€ Test_pdf/               # PDF files
â”‚   â””â”€â”€ enum_types/             # Enum CSV files
â”œâ”€â”€ output_1970/                # Output à¸ªà¸³à¸«à¸£à¸±à¸š nacc_id=1970
â”‚   â”œâ”€â”€ submitter_old_name.csv
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ summary.csv
â”œâ”€â”€ output_2448/                # Output à¸ªà¸³à¸«à¸£à¸±à¸š nacc_id=2448
â””â”€â”€ submission_summary.csv      # à¹„à¸Ÿà¸¥à¹Œà¸ªà¸³à¸«à¸£à¸±à¸š submit
```

---

## âš¡ Quick Start

```python
# Main Pipeline
def process_all_documents():
    """
    à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ PDF à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
    """
    
    # 1. à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
    doc_info = pd.read_csv("Test_doc_info.csv", encoding="utf-8-sig")
    nacc_detail = pd.read_csv("Test_nacc_detail.csv", encoding="utf-8-sig")
    submitter_info = pd.read_csv("Test_submitter_info.csv", encoding="utf-8-sig")
    
    # 2. à¸§à¸™à¸¥à¸¹à¸›à¸—à¸¸à¸ PDF
    for idx, doc_row in doc_info.iterrows():
        pdf_filename = doc_row['doc_location_url']
        nacc_id = doc_row['nacc_id']
        
        print(f"\n{'='*80}")
        print(f"Processing {idx+1}/{len(doc_info)}: {pdf_filename}")
        print(f"{'='*80}")
        
        # 3. PDF â†’ Images
        images = pdf_to_images(f"./Test_pdf/{pdf_filename}")
        
        # 4. OCR
        full_text = ""
        for img in images:
            text = ocr_image(img)
            full_text += text + "\n\n"
        
        # 5. LLM Parsing
        parsed_results = parse_all_sections(full_text, nacc_id)
        
        # 6. Generate CSV
        generate_csv_files(parsed_results, nacc_id)
        
        # 7. Validate
        validate_and_create_summary(Path(f"output_{nacc_id}"))
    
    # 8. Create final submission
    prepare_submission()
    
    print("\nâœ… à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™!")

# à¸£à¸±à¸™!
if __name__ == "__main__":
    process_all_documents()
```

---

## âœ… à¸ªà¸£à¸¸à¸›

**à¹ƒà¸Šà¹ˆà¹à¸¥à¹‰à¸§à¸„à¸£à¸±à¸š! Flow à¸„à¸·à¸­:**

1. âœ… à¹€à¸£à¸´à¹ˆà¸¡à¸ˆà¸²à¸ `Test_doc_info.csv` â†’ à¹„à¸”à¹‰à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­ PDF
2. âœ… à¸­à¹ˆà¸²à¸™ PDF â†’ à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ Images
3. âœ… OCR (Tesseract) â†’ à¹„à¸”à¹‰ Text
4. âœ… LLM Parsing â†’ à¹„à¸”à¹‰ Structured Data à¸à¸£à¹‰à¸­à¸¡ Enum ID
5. âœ… Generate CSV â†’ à¹„à¸”à¹‰ 13 à¹„à¸Ÿà¸¥à¹Œ
6. âœ… Run validation_query.sql â†’ à¹„à¸”à¹‰ summary.csv
7. âœ… Submit to Kaggle

**à¸„à¸µà¸¢à¹Œà¸ªà¸³à¸„à¸±à¸:**
- ğŸ“Œ à¹ƒà¸Šà¹‰ `nacc_id` à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¹€à¸Šà¸·à¹ˆà¸­à¸¡
- ğŸ“Œ à¹ƒà¸Šà¹‰ enum ID à¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡ (à¸«à¹‰à¸²à¸¡à¹€à¸”à¸²!)
- ğŸ“Œ UTF-8-sig encoding à¸ªà¸³à¸«à¸£à¸±à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢
- ğŸ“Œ Temperature=0.1 à¸ªà¸³à¸«à¸£à¸±à¸š LLM à¹ƒà¸«à¹‰ consistent